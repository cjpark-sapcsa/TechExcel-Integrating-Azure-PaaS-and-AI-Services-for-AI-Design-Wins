---
title: '2. Enable semantic caching for Azure OpenAI APIs'
layout: default
nav_order: 2
parent: 'Exercise 06: Add GenAI Gateway Capabilities with Azure API Management'
---

## Introduction

By enabling semantic caching of responses to Azure OpenAI API requests, you can reduce bandwidth and processing requirements imposed on the backend APIs and lower the latency perceived by API consumers. With semantic caching, you can return cached responses for identical prompts and also for prompts that are similar in meaning, even if the text isn't the same.

## Description

In this task, you enable semantic caching for Azure OpenAI APIs using Azure API Management (APIM) and Azure Cache for Redis.


 as a semantic cache with Azure OpenAI Service to generate LLM responses to queries and cache those responses using Azure Cache for Redis, delivering faster responses and lowering costs. Using its built-in vector search capability, you will also perform semantic caching using Azure Cache for Redis. You will evaluate the performance benefits of returning cached responses for queries.

The key tasks are as follows:

- Add Azure Cache for Redis as an external cache in APIM.
- Enable semantic caching of responses to Azure OpenAI API requests.
- Use Azure OpenAI service to generate text from AI models and cache results.
- See the performance gains from using caching with LLMs.

## Success Criteria

- You have added the Azure OpenAI Text Embedding Generation service to the Kernel builder in the `Program.cs` file.
- You have updated the `VectorizationService` and its `GetEmbeddings` method to use the built-in text embedding generation service to create vector embeddings for query text.
- You have successfully executed a query against the `Vectorize` and `/VectorSearch` endpoints of the web API using the Streamlit dashboard's **Vector Search** page.

## Learning Resources

- [Add caching to improve performance in Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-howto-cache)
- [Enable semantic caching for Azure OpenAI APIs in Azure API Management](https://learn.microsoft.com/azure/api-management/azure-openai-enable-semantic-caching)
- [Use an external Redis-compatible cache in Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-howto-cache-external)
- [Use Azure Cache for Redis as a semantic cache](https://learn.microsoft.com/azure/azure-cache-for-redis/cache-tutorial-semantic-cache)
- [Cache responses to Azure OpenAI API requests](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-store-policy)
- [Get cached responses of Azure OpenAI API requests](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy)

## Solution

<details markdown="block">
<summary>Expand this section to view the solution</summary>

- To use Azure Cache for Redis as an external cache in Azure API Management:
  - Before adding your external cache, you will need to retrieve the endpoint and key of your Azure Cache for Redis instance. In the [Azure portal](https://portal.azure.com/), navigate to your Redis Enterprise cluster resource.
  - On the **Overview** page, copy the **Endpoint** value and save it into a text editor, such as Notepad, for later use.
  - Open the **Access keys** page from the **Settings** menu, then copy the **Primary** key value, and paste it into a text edit for later use.
  - Next, navigate to your API Management service in the [Azure portal](https://portal.azure.com/).
  - Select **External cache** from the **Deployment + infrastructure** menu, then select **Add** on the External cache page.

    ![Screenshot of the APIM external cache page, with the External cache menu item and the Add button highlighted.](../../media/Solution/0602-apim-add-external-cache.png)

  - In the **External cache** dialog:
    - Choose your Redis Enterprise cluster resource from the **Cache instance** dropdown.

        {: .note }
        > You may receive an error at this point stating, "the requested operation failed because you don't have sufficient permissions for the action: 'https:/listSecrets'." You can safe ignore this error, as you will manually provide the connection string below.

    - Select the region you are using for resources in this exercise in the **Use from** dropdown. This will be selected by default, but if not, it is labeled with "(managed)" in the list.
    - In the **Connnection string** box, paste the following value, replacing the `<YOUR_AZURE_CACHE_FOR_REDIS_ENDPOINT>` and `<YOUR_AZURE_CACHE_FOR_REDIS_PRIMARY_KEY>` with the endpoint and key of your Azure Cache for Redis instance, respectively.

    ```sql
    <YOUR_AZURE_CACHE_FOR_REDIS_ENDPOINT>,password=<YOUR_AZURE_CACHE_FOR_REDIS_PRIMARY_KEY>=,ssl=True,abortConnect=False
    ```

  - Select **Save**.

- To configure a semantic caching policy...
  - First, you must configure a backend for the embeddings API deployment:
    - Navigate to the **Backends** page for your API Management service, which is accessible from the **APIs** menu.
    - Select **Add**.
    - On the **Backend** dialog:
      - Enter **embeddings-backend** for the Name.
      - Choose **Custom URL** for the Type.
      - Enter the following value into the **Runtime URL** box, replacing `<YOUR_AZURE_OPENAI_ENDPOINT>` with the endpoint value for your Azure OpenAI service. You can find the endpoint value on the **Keys and Endpoint** page for your Azure OpenAI service in the [Azure portal](https://portal.azure.com/).

        ```sql
        `<YOUR_AZURE_OPENAI_ENDPOINT>openai/deployments/text-embedding-ada-002/embeddings`
        ```

      - Select **Create**.
  - Next, navigate to the **APIs** page of your API Management service in the [Azure portal](https://portal.azure.com/) and select the **Azure OpenAI API**.
    - Select **All operations** in the design panel, then open the inbound processing policies by selecting the `<\>` link in that panel.

        ![Screenshot of the design page of the Azure OpenAPI API in APIM, with the API name, All operations page, and policies link highlighted.](../../media/Solution/0602-apim-azure-openai-api-all-operations-inbound-processing-policies.png)

    - In the **Policies** XML document, add a policy to enable semantic caching by adding the following `azure-openai-semantic-cache-lookup` policy within the `<inbound>` section:

        ```xml
        <azure-openai-semantic-cache-lookup
            score-threshold="0.85"
            embeddings-backend-id="embeddings-backend"
            embeddings-backend-auth="system-assigned"
            ignore-system-messages="true"
            max-message-count="10" />
        ```

    - In the same **Policies** XML document, insert the following `azure-openai-semantic-cache-store` policy into the `<outbound>` section:

        ```xml
        <azure-openai-semantic-cache-store duration="60" />
        ```

    - Select **Save**. Your final policies document will look like:

        ```xml
        <policies>
            <inbound>
                <set-backend-service id="apim-generated-policy" backend-id="azure-openai-api-openai-endpoint" />
                <azure-openai-token-limit tokens-per-minute="500" counter-key="@(context.Request.IpAddress)" estimate-prompt-tokens="true" tokens-consumed-header-name="consumed-tokens" remaining-tokens-header-name="remaining-tokens" />
                <azure-openai-emit-token-metric>
                    <dimension name="API ID" />
                    <dimension name="Client IP address" value="@(context.Request.IpAddress)" />
                </azure-openai-emit-token-metric>
                <azure-openai-semantic-cache-lookup score-threshold="0.85" embeddings-backend-id="embeddings-backend" embeddings-backend-auth="system-assigned" ignore-system-messages="true" max-message-count="10" />
                <authentication-managed-identity resource="https://cognitiveservices.azure.com/" />
                <base />
            </inbound>
            <backend>
                <base />
            </backend>
            <outbound>
                <azure-openai-semantic-cache-store duration="60" />
                <base />
            </outbound>
            <on-error>
                <base />
            </on-error>
        </policies>
        ```

- Confirm semantic caching is working by executing a chat completion request.
  - Select the **Test** tab on the API design page, then select the **Creates a completion for the chat message** endpoint.
  - On the **Creates a completion for the chat message** page, enter the following under **Template parameters**:
    - **deployment-id**: Enter "gpt-4o"
    - **api-version**: Enter "2024-06-01"
  - Scroll down to the **Request body** section, ensure **Raw** is selected, and paste the following into the text box.

    ```json
    {
        "messages":[
            {"role": "system", "content": "Echo the user prompt."},
            {"role": "user", "content": "Tell me an interesting story about AI."}
        ]
    }
    ```

  - Select **Send** at the bottom of the page.
  - Scroll to the page's **HTTP response** section and ensure you received a `200 OK` response, indicated at the top of the headers.
  - Next, repeat the request, but this time use the **Trace** button to send the request.
  - In the **HTTP response** section, select the **Trace** tab, then scroll to bottom of the **Inbound** section, and locate the following sections named `OpenAISemanticCacheLookupPolicy` and `azure-openai-semantic-cache-store`.

   The `OpenAISemanticCacheLookupPolicy` section will indicate if a matching cache entry was found and the vector distance of the match, such as:

   ```json
   {
      "message": "Found cache entry using semantic search within threshold of '0.85', and similarity score '1.0000001' under the vary-by partition 'None'. Vector distance is '-1.1920929E-07'."
   }
   ```

   The `azure-openai-semantic-cache-store` section provides details about the matched cached key, like the following:

   ```json
   {
       "message": "Cache lookup resulted in a hit! Cached response will be used. Processing will continue from the step in the response pipeline that is after the corresponding `cache-store`.",
       "cacheKey": "ApimSemanticCache:V1:Dim1536:eastus2preprobv27853a0d5akdtwebvz3bjoiko0fs8otzsfs.655045:azure-openai-api;rev=1.655351:ChatCompletions_Create:4::c10c47b8-696f-4997-adfc-0d180f52fadc"
   }
   ```

   {: .note }
   > If you do not see that it found a cache entry, try sending the request again, as the cache has a 60 second storage time and it may have disappeared before you sent the second request.

  - Return to the **Messages** tab, locate the `consumed-tokens` header, and observe the value. When the user prompt has been cached, the value should be `0`... TODO: Verify this.

  - Next, execute a variation of the query by sending in the following request body:

    ```json
    {
        "messages":[
            {"role": "system", "content": "Echo the user prompt."},
            {"role": "user", "content": "Tell me an detailed story about AI."}
        ]
    }
    ```

  - The text of the request is not the same, but is semantically similar, so you should still see a match in the cache. TODO: Add a bit more here about semantic caching and why this matched...

</details>